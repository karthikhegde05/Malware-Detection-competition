# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

input_dir = '/kaggle/input/iiitb-ml-malware-detection-tanmay/'

train = pd.read_csv(input_dir + 'Malware_train.csv')
test = pd.read_csv(input_dir + 'Malware_test.csv')

# pre-processing

# listing columns having missing values
miss_val_train = train.isnull().sum()
miss_val_per_train = train.isnull().sum()*100/(train.shape[0])

miss_val_train_table = pd.concat([miss_val_train, miss_val_per_train], axis=1)
miss_table = miss_val_train_table.rename(columns={0: 'Missing values', 1:'missing percentage of total values'})


#dropping the columns having high percentage of missing values
lst_tobeDropped_col = miss_table.loc[miss_table['missing percentage of total values']>50].index
train = train.drop(lst_tobeDropped_col, axis='columns')
test = test.drop(lst_tobeDropped_col, axis='columns')



# encoding categorical columns
# listing categorical columns
categorical_cols = list(train.select_dtypes(['object']).columns)

col_with_huge_unique_val = ['MachineIdentifier', 'AvSigVersion', 'OsBuildLab', 'Census_OSVersion']

# removing columns having large number of unique values from the list of categorical columns
for col in col_with_huge_unique_val:
    categorical_cols.remove(col)
    
lst_num = train.select_dtypes(['int', 'float']).columns #includes the target col - 'HasDetections'

# listing numeric columns having small number of unique values
num_col_with_small_categories = []
for col in lst_num:
    if(train[col].nunique()<20 and col!='HasDetections'):
        num_col_with_small_categories.append(col)
        
# considerin the numeric columns having small number of unique values as categorical columns        
for col in num_col_with_small_categories:
    categorical_cols.append(col)

# type conversion to str    
for col in categorical_cols:
    train[col] = train[col].astype(str)
    test[col] = test[col].astype(str)
    
    
# Some categorical columns have extra classes in test set which are absent in train set.
# listing such columns and their extra classes
lst_extra_encodings = []
extra_encoding_col = []
for col in categorical_cols:
    lst1 = set(train[col].unique())
    lst2 = set(test[col].unique())
    tmp = lst2 - lst1
    if(len(tmp)>0):
        extra_encoding_col.append(col)
    for el in tmp:
        lst_extra_encodings.append(el)
        
        
# Label Encoding
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

i = 0
for col in categorical_cols:
    label_encoder.fit(train[col])
    i+=1
    if(col=='EngineVersion'):
        label_encoder.classes_ = np.append(label_encoder.classes_, '1.1.11602.0') # count = 1 in test set
        label_encoder.classes_ = np.append(label_encoder.classes_, '1.1.12002.0') # count = 1 in test set
    if(col=='AppVersion'):
        label_encoder.classes_ = np.append(label_encoder.classes_, '4.18.1806.20015') # count = 1 in test set
        label_encoder.classes_ = np.append(label_encoder.classes_, '4.11.15063.1154') # count = 1 in test set
        label_encoder.classes_ = np.append(label_encoder.classes_, '4.12.17007.18021') # count = 1 in test set
        label_encoder.classes_ = np.append(label_encoder.classes_,  '4.13.17627.1000') # count = 1 in test set
        label_encoder.classes_ = np.append(label_encoder.classes_,  '4.9.10586.1177') # count = 1 in test set
    if(col=='OsVer'):
        label_encoder.classes_ = np.append(label_encoder.classes_, '10.0.0.22') # count = 1 in test set
        label_encoder.classes_ = np.append(label_encoder.classes_, '6.3.16.0') # count = 1 in test set
    if(col=='SmartScreen'):
        label_encoder.classes_ = np.append(label_encoder.classes_, '0') # count = 1 in test set
    if(col=='Census_FlightRing'):
        label_encoder.classes_ = np.append(label_encoder.classes_, 'OSG') # count = 1 in test set
    if(col=='OsSuite'):
        label_encoder.classes_ = np.append(label_encoder.classes_, '49') # count = 1 in test set
    train[col] = label_encoder.transform(train[col])
    test[col] = label_encoder.transform(test[col])
    

# imputation of missing values
    
# listing columns other than the categorical columns
continuous_cols = []

for col in train.columns:
    if(col in categorical_cols):
        continue
    if(train[col].dtype != 'object' and col!='HasDetections'):
        continuous_cols.append(col)

# imputing the missing values in continuous columns with the mean values        
for col in continuous_cols:
    if(train[col].isnull().sum()>0):
        train[col] = train[col].fillna(train[col].mean())
        test[col] = test[col].fillna(test[col].mean())


# saving MachineIdentifier column for later submission

Id_train_col = train['MachineIdentifier']
Id_test_col = test['MachineIdentifier']

# dropping the columns with large set of unique values
col_with_huge_unique_val = ['MachineIdentifier', 'AvSigVersion', 'OsBuildLab', 'Census_OSVersion']
train = train.drop(col_with_huge_unique_val, axis='columns')
test = test.drop(col_with_huge_unique_val, axis='columns')

# listing the features to train on. 
features_to_train_on = list(train.columns)
features_to_train_on.remove('HasDetections') # The target column is not included in the list of features

# list of top 12 features extracted from earlier XGBoost model

list_from_feature_imp = ['AVProductsInstalled',
 'SmartScreen',
 'Census_IsVirtualDevice',
 'EngineVersion',
 'Processor',
 'IsProtected',
 'Census_OSArchitecture',
 'AVProductStatesIdentifier',
'AppVersion',
 'Wdft_IsGamer',
 'Census_TotalPhysicalRAM',
 'RtpStateBitfield']


# now passing these features to polynomial feature transformer

from sklearn.preprocessing import PolynomialFeatures
                                  
# Create the polynomial object with specified degree
poly_transformer = PolynomialFeatures(degree = 2)

poly_features_train = train[list_from_feature_imp]
poly_features_test = test[list_from_feature_imp]

poly_transformer.fit(poly_features_train)

poly_features_train = poly_transformer.transform(poly_features_train)
poly_features_test = poly_transformer.transform(poly_features_test)


# renaming the columns of poly_features 
poly_features_train = pd.DataFrame(poly_features_train, columns=poly_transformer.get_feature_names(list_from_feature_imp))
poly_features_test = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names(list_from_feature_imp))

# ignoring the first column - ('1' degree 0 variable)
poly_features_train = poly_features_train.iloc[:, 1:]
poly_features_test = poly_features_test.iloc[:, 1:]


# drop the original columns and later we will add the features generated from polynomial transformer.
train = train.drop(list_from_feature_imp, axis='columns')
test = test.drop(list_from_feature_imp, axis='columns')


# adding the features generated from polynomial transformer
# these include the original features as well 
train = pd.concat([train, poly_features_train], axis=1)
test = pd.concat([test, poly_features_test], axis=1)


# listing the new features to train on
new_features_to_train_on = list(train.columns)
new_features_to_train_on.remove('HasDetections')

X = train[new_features_to_train_on]
y = train['HasDetections']


# Model selection and training

import xgboost as xgb


# listing the hyper-parameters for the XGBoost model and instantiation
xgb_model = xgb.XGBClassifier(objective='binary:logistic', 
                                     tree_method = 'gpu_hist', # training on gpu 
                                      eval_metric='auc',
                                      seed=5,
                                      eta=0.1,
                                      max_depth=6,
                                      n_estimators = 1000,
                                      scale_pos_weight = 1,
                                      reg_lambda = 295,
                                      n_jobs=-1)


from sklearn.model_selection import cross_val_predict, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score 

# Using Stratified K fold 
skf = StratifiedKFold(n_splits=5, random_state=3)


# Cross Validation
# updating auc scores at every iteration and taking the mean
cross_val_auc = []

# updating training auc scores at every iteration for comparison
training_auc = []

for train_index, validation_index in skf.split(X, y):
    
    X_train_subset = X.iloc[train_index]
    y_train_subset = y.iloc[train_index]
    X_validation_subset = X.iloc[validation_index]
    y_validation_subset = y.iloc[validation_index]
    
    # fitting the model on train subset
    xgb_model.fit(X_train_subset, y_train_subset) 
    
    # predicting on the train subset  
    train_subset_pred_proba = xgb_model.predict_proba(X_train_subset)
    
    # predicting on the validation subset 
    validation_pred_proba = xgb_model.predict_proba(X_validation_subset)
    
    # updating roc_auc_scores
    training_auc.append(roc_auc_score(y_train_subset, train_subset_pred_proba[:, 1]))
    cross_val_auc.append(roc_auc_score(y_validation_subset, validation_pred_proba[:, 1]))
    
#print('training roc_auc_score: {}'.format(np.mean(training_auc)))    
#print ('Cross Validated roc_auc_score: {}'.format(np.mean(cross_val_auc)))



# training the best model, from cross validation scores, on the entire train data
# training on the entire train data for better submission results
xgb_model.fit(X, y)



# predicting probabilities on the test set
test_pred_proba = xgb_model.predict_proba(test)
submission_pred = pd.Series(test_pred_proba[:, 1], name='HasDetections')

# concating the 'MachineIdentifier' to the prediction table as it is required for the submission
submission_table = pd.concat([Id_test_col, submission_pred], axis=1)
output_path = 'submission11.csv'

# converting to csv file for submission
submission_table.to_csv(output_path, index=False)
